{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landscape elevational connectivity analyse\n",
    "\n",
    "Here we perform the same operation as what is done in notebook `1-computeLEC` but a series of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cmocean as cmo\n",
    "\n",
    "from pyevtk.hl import gridToVTK\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action = \"ignore\", category = FutureWarning)\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineDataset(liststep=None,catchmentID=None,folder=None,filename=None,gridExt='.csv',LECExt='LEC.csv',\n",
    "                   combinedExt='comb.csv'):\n",
    "    \n",
    "\n",
    "    for k in range(len(liststep)):\n",
    "        step = liststep[k]\n",
    "        file1 = folder+'/'+filename+str(step)+gridExt\n",
    "        file2 = folder+'/'+filename+str(step)+LECExt\n",
    "        outfile = folder+'/'+filename+str(step)+combinedExt\n",
    "        viewfile = folder+'/'+filename+str(step) \n",
    "\n",
    "        # First the elevation file\n",
    "        df = pd.read_csv(file1, sep=',', engine='c',\n",
    "                        header=0, na_filter=False, dtype=np.float,\n",
    "                        low_memory=False)\n",
    "        X = df['X']\n",
    "        Y = df['Y']\n",
    "        Z = df['Z']\n",
    "        B = df['Basin']\n",
    "        dx = ( X[1] - X[0] )\n",
    "        nx = int((X.max() - X.min())/dx+1)\n",
    "        ny = int((Y.max() - Y.min())/dx+1)\n",
    "\n",
    "        # Second the LEC file\n",
    "        df2 = pd.read_csv(file2, sep=',', engine='c',\n",
    "                        header=0, na_filter=False, dtype=np.float,\n",
    "                        low_memory=False)\n",
    "        LEC = df2['LEC']\n",
    "        X = np.reshape(X,(ny,nx))\n",
    "        Y = np.reshape(Y,(ny,nx))\n",
    "        Z = np.reshape(Z,(ny,nx))\n",
    "        Basin = np.reshape(B,(ny,nx))\n",
    "        LEC = np.reshape(LEC,(ny,nx))\n",
    "        nLEC = LEC/LEC.max()\n",
    "\n",
    "        # Then write the combined dataset\n",
    "        df3 = pd.DataFrame({'X':X.flatten(),'Y':Y.flatten(),\n",
    "                            'Z':Z.flatten(),'LEC':LEC.flatten(), 'nLEC':nLEC.flatten(), 'Basin':Basin.flatten()})    \n",
    "        df3.to_csv(outfile, columns=['X', 'Y', 'Z', 'LEC', 'nLEC', 'Basin'], sep=',', index=False , header=1)\n",
    "\n",
    "        # Finally create a vtk file for visualisation\n",
    "        Xv = np.zeros((X.shape[0],X.shape[1],2))\n",
    "        Yv = np.zeros((X.shape[0],X.shape[1],2))\n",
    "        Zv = np.zeros((X.shape[0],X.shape[1],2))\n",
    "        LECv = np.zeros((X.shape[0],X.shape[1],2))\n",
    "        normLEC = np.zeros((X.shape[0],X.shape[1],2))\n",
    "        Basinv = np.zeros((X.shape[0],X.shape[1],2))\n",
    "        Xv[:,:,0] = X\n",
    "        Yv[:,:,0] = Y\n",
    "        Zv[:,:,0] = Z\n",
    "        LECv[:,:,0] = LEC\n",
    "        normLEC[:,:,0] = LEC/LEC.max()\n",
    "        Basinv[:,:,0] = Basin\n",
    "        Xv[:,:,1] = X\n",
    "        Yv[:,:,1] = Y\n",
    "        Zv[:,:,1] = Z\n",
    "        LECv[:,:,1] = LEC\n",
    "        normLEC[:,:,1] = LEC/LEC.max()\n",
    "        Basinv[:,:,1] = Basin\n",
    "        gridToVTK(viewfile, Xv, Yv, Zv, pointData = {\"elevation\" : Zv, \"LEC\" :LECv, \"nLEC\" :normLEC, \"BasinID\" :Basinv})\n",
    "        \n",
    "        if catchmentID is not None:\n",
    "            basinID = catchmentID[k]\n",
    "            basinfile = folder+'/'+filename+str(step)+'basin'+combinedExt\n",
    "            bIDr,bIDl = np.where(Basin==basinID)\n",
    "            df4 = pd.DataFrame({'X':X[bIDr,bIDl].flatten(),'Y':Y[bIDr,bIDl].flatten(),\n",
    "                                'Z':Z[bIDr,bIDl].flatten(),'LEC':LEC[bIDr,bIDl].flatten(), \n",
    "                                'nLEC':nLEC[bIDr,bIDl].flatten()})    \n",
    "            df4.to_csv(basinfile, columns=['X', 'Y', 'Z', 'LEC', 'nLEC'], sep=',', index=False , header=1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the folder and file names and extensions that will be read and writen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'output'\n",
    "filename = 'grid'\n",
    "gridExt = '.csv'\n",
    "LECExt = 'LEC-con.csv'\n",
    "combinedExt = 'comb.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then specify the time steps that will be analysed and if specific catchment values need to be extracted the corresponding temporal catchment IDs needs to be given based on notebook `0-regridBadlands`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liststep = np.linspace(5,200,40).astype(int)\n",
    "catchmentID = [372,180,260,253,87,7,243,17,341,311,61,238,216,283,191,191,\n",
    "               313,36,121,235,223,212,341,17,307,227,171,225,205,220,54,47,\n",
    "               65,118,237,199,119,274,164,175]\n",
    "\n",
    "liststepOro = np.linspace(5,180,36).astype(int)\n",
    "catchmentIDOro = [172,75,76,304,357,305,321,123,174,34,98,212,354,351,237,304,\n",
    "                  174,199,368,63,364,71,10,23,58,217,65,63,327,35,286,53,312,\n",
    "                  320,61,138]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we run the `combineDataset` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combineDataset(liststep,catchmentID,folder,filename,gridExt,LECExt,combinedExt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting elevation\n",
    "\n",
    "Analyse of elevation for a given time step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 100\n",
    "folder = 'outputKe'\n",
    "filename = 'grid'\n",
    "combinedExt = 'comb.csv'\n",
    "loadfile = folder+'/'+filename+str(step)+combinedExt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(loadfile, sep=',', engine='c',\n",
    "                header=0, na_filter=False, dtype=np.float,\n",
    "                low_memory=False)\n",
    "X = df['X']\n",
    "Y = df['Y']\n",
    "Z = df['Z']\n",
    "dx = ( X[1] - X[0] )\n",
    "nx = int((X.max() - X.min())/dx+1)\n",
    "ny = int((Y.max() - Y.min())/dx+1)\n",
    "#\n",
    "Z = np.reshape(Z,(ny,nx))\n",
    "print 'Resolution ',dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotElev(elev=None,color=cmo.cm.delta,title='Elevation',figsize=(6,6),extend=None,save=None):\n",
    "    \n",
    "    save = None\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax1 = plt.gca()\n",
    "\n",
    "    ax1.set_title(title, fontsize=10)\n",
    "    if extend is None:\n",
    "        im1 = plt.imshow(np.flipud(elev), vmin=elev.min(), vmax=elev.max(), \n",
    "                         cmap=color, aspect='auto')\n",
    "    else:\n",
    "        im1 = plt.imshow(np.flipud(elev[extend[0]:extend[1],extend[2]:extend[3]]), \n",
    "                         vmin=elev.min(), vmax=elev.max(), \n",
    "                         cmap=color, aspect='auto')\n",
    "        \n",
    "    ax1.tick_params(axis='x', labelsize=8)\n",
    "    ax1.tick_params(axis='y', labelsize=8)\n",
    "    ax1.grid(False)\n",
    "\n",
    "    divider1 = make_axes_locatable(ax1)\n",
    "    cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar1 = plt.colorbar(im1,cax=cax1)\n",
    "    cbar1.ax.tick_params(labelsize=9) \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    if save is not None:\n",
    "        fig.savefig(save,dpi=200, bbox_inches='tight')\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualise the elevation using the `plotElev` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotElev(elev=Z,color=cmo.cm.delta,title='Elevation',figsize=(6,6),extend=None,save=None)\n",
    "plotElev(elev=Z,color=cmo.cm.delta,title='Elevation',figsize=(6,6),extend=[330,430,150,250],save=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting a sub-domain from the region\n",
    "\n",
    "We first define the row/column limits of each sub-domain of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domainA = [60,110,80,130]\n",
    "domainB = [230,330,200,300]\n",
    "domainC = [330,430,150,250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeSubDomain(filename=None,domain=None):\n",
    "    \n",
    "        df = pd.DataFrame({'X':X[domain[0]:domain[1],domain[2]:domain[3]].flatten(),\n",
    "                           'Y':Y[domain[0]:domain[1],domain[2]:domain[3]].flatten(),\n",
    "                           'Z':Z[domain[0]:domain[1],domain[2]:domain[3]].flatten(),\n",
    "                           'LEC':LEC[domain[0]:domain[1],domain[2]:domain[3]].flatten(),\n",
    "                           'nLEC':nLEC[domain[0]:domain[1],domain[2]:domain[3]].flatten()}) \n",
    "        \n",
    "        df.to_csv(filename, columns=['X', 'Y', 'Z', 'LEC', 'nLEC'], sep=',', index=False , header=1)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 100\n",
    "folder = 'outputKe'\n",
    "filename = 'grid'\n",
    "combinedExt = 'comb.csv'\n",
    "loadfile = folder+'/'+filename+str(step)+combinedExt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(loadfile, sep=',', engine='c',\n",
    "                header=0, na_filter=False, dtype=np.float,\n",
    "                low_memory=False)\n",
    "X = df['X']\n",
    "Y = df['Y']\n",
    "Z = df['Z']\n",
    "B = df['Basin']\n",
    "LEC = df['LEC']\n",
    "nLEC = df['nLEC']\n",
    "\n",
    "dx = ( X[1] - X[0] )\n",
    "nx = int((X.max() - X.min())/dx+1)\n",
    "ny = int((Y.max() - Y.min())/dx+1)\n",
    "\n",
    "X = np.reshape(X,(ny,nx))\n",
    "Y = np.reshape(Y,(ny,nx))\n",
    "Z = np.reshape(Z,(ny,nx))\n",
    "Basin = np.reshape(B,(ny,nx))\n",
    "LEC = np.reshape(LEC,(ny,nx))\n",
    "nLEC = np.reshape(nLEC,(ny,nx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And wrote sub-domain values to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileA = folder+'/'+filename+str(step)+'domainA.csv'\n",
    "fileB = folder+'/'+filename+str(step)+'domainB.csv'\n",
    "fileC = folder+'/'+filename+str(step)+'domainC.csv'\n",
    "writeSubDomain(filename=fileA,domain=domainA)\n",
    "writeSubDomain(filename=fileB,domain=domainB)\n",
    "writeSubDomain(filename=fileC,domain=domainC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEC & Elevation distribution\n",
    "\n",
    "_Elevation frequency as function of site elevation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "step = 100\n",
    "folder = 'outputKe'\n",
    "filename = 'grid'\n",
    "combinedExt = 'comb.csv'\n",
    "loadfile = folder+'/'+filename+str(step)+combinedExt\n",
    "\n",
    "data = pd.read_csv(loadfile)\n",
    "\n",
    "ax = data.Z.plot(kind='hist', color='Blue', alpha=0.1, bins=80, xlim=(data.Z.min(),data.Z.max()))\n",
    "plt.xlabel('Elevation')\n",
    "data.Z.plot(kind='density', figsize=(10,6), ax=ax, xlim=(data.Z.min(),data.Z.max()),\n",
    "               linewidth=4,title='Elevation frequency as function of site elevation',secondary_y=True,y='Density')\n",
    "ax.set_ylabel('Frequency count')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Landscape elevational connectivity as a function of elevation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = data.plot(kind='scatter', x='Z', y='LEC', c='white', edgecolors='lightgray', figsize=(10,6), \n",
    "          xlim=(data['Z'].min(),data['Z'].max()), s=5, title='Landscape elevational connectivity periodic boundary fully connected')\n",
    "plt.xlabel('Elevation')\n",
    "ax2=ax.twinx()  \n",
    "data.Z.plot(kind='density',secondary_y=True, ax=ax2, xlim=(data['Z'].min(),data['Z'].max()),\n",
    "               linewidth=2)\n",
    "ax.set_ylabel('LEC')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Combined plots_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 5\n",
    "folder = 'outputKe'\n",
    "filename = 'grid'\n",
    "combinedExt = 'comb.csv'\n",
    "loadfile = folder+'/'+filename+str(step)+combinedExt\n",
    "\n",
    "data = pd.read_csv(loadfile)\n",
    "\n",
    "nbins = 40\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "w = data.nLEC\n",
    "n, _ = np.histogram(data.Z, bins=nbins)\n",
    "sy, _ = np.histogram(data.Z, bins=nbins, weights=w)\n",
    "sy2, _ = np.histogram(data.Z, bins=nbins, weights=w*w)\n",
    "mean = sy / n\n",
    "std = np.sqrt(sy2/n - mean*mean)\n",
    "\n",
    "plt.ylabel('normalised LEC')\n",
    "plt.xlabel('Elevation')\n",
    "\n",
    "plt.plot((_[1:] + _[:-1])/2, mean,color='steelblue',zorder=2,linewidth=2)\n",
    "\n",
    "num_samples = 25000\n",
    "idx = np.random.choice(np.arange(len(data.Z)), num_samples)\n",
    "plt.scatter(data.Z[idx], w[idx], c='w',edgecolors='lightgray', zorder=0,alpha=1.,s=5)\n",
    "ax.set_xlim(data['Z'].min(),data['Z'].max())\n",
    "ax.set_ylim(0,1)\n",
    "\n",
    "(_, caps, _) = plt.errorbar(\n",
    "    (_[1:] + _[:-1])/2, mean, yerr=std, fmt='-o', c='steelblue',markersize=4, capsize=3,zorder=1,linewidth=1.25)\n",
    "\n",
    "for cap in caps:\n",
    "    cap.set_markeredgewidth(1)\n",
    "\n",
    "\n",
    "ax2=ax.twinx()  \n",
    "data.Z.plot(kind='density',secondary_y=True, ax=ax2, xlim=(data['Z'].min(),data['Z'].max()),\n",
    "               color='coral',linewidth=2,zorder=0)\n",
    " \n",
    "\n",
    "plt.ylabel('Density', color='coral')\n",
    "# plt.show()\n",
    "\n",
    "figname = folder+'/'+filename+str(step)+'.png'\n",
    "# fig.savefig(figname, dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpl_scatter_density\n",
    "\n",
    "step = 45\n",
    "folder = 'outputKe'\n",
    "filename = 'grid'\n",
    "combinedExt = 'comb.csv'\n",
    "loadfile = folder+'/'+filename+str(step)+combinedExt\n",
    "\n",
    "data = pd.read_csv(loadfile)\n",
    "\n",
    "nbins = 40\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax11 = fig.add_subplot(1, 1, 1, projection='scatter_density')\n",
    "\n",
    "w = data.nLEC\n",
    "n, _ = np.histogram(data.Z, bins=nbins)\n",
    "sy, _ = np.histogram(data.Z, bins=nbins, weights=w)\n",
    "sy2, _ = np.histogram(data.Z, bins=nbins, weights=w*w)\n",
    "mean = sy / n\n",
    "std = np.sqrt(sy2/n - mean*mean)\n",
    "\n",
    "plt.ylabel('normalised LEC')\n",
    "plt.xlabel('Elevation')\n",
    "#mediumseagreen\n",
    "plt.plot((_[1:] + _[:-1])/2, mean,color='k',zorder=2,linewidth=2)\n",
    "\n",
    "num_samples = 25000\n",
    "idx = np.random.choice(np.arange(len(data.Z)), num_samples)\n",
    "#plt.scatter(data.Z[idx], w[idx], c='w',edgecolors='lightgray', zorder=0,alpha=1.,s=5)\n",
    "ax11.scatter_density(data.Z, w, cmap=cmo.cm.gray_r) #color='grey')\n",
    "ax.set_xlim(data['Z'].min(),data['Z'].max())\n",
    "ax.set_ylim(0,1)\n",
    "ax11.set_ylim(0,1)\n",
    "\n",
    "(_, caps, _) = plt.errorbar(\n",
    "    (_[1:] + _[:-1])/2, mean, yerr=std, fmt='-o', c='k',markersize=4, capsize=3,zorder=1,linewidth=1.25)\n",
    "\n",
    "for cap in caps:\n",
    "    cap.set_markeredgewidth(1)\n",
    "\n",
    "\n",
    "ax2=ax.twinx()  \n",
    "data.Z.plot(kind='density',secondary_y=True, ax=ax2, xlim=(data['Z'].min(),data['Z'].max()),\n",
    "               color='coral',linewidth=2,zorder=0)\n",
    " \n",
    "\n",
    "plt.ylabel('Density', color='coral')\n",
    "# plt.show()\n",
    "\n",
    "figname = folder+'/'+filename+str(step)+'.png'\n",
    "fig.savefig(figname, dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
